# transformer_from_scratch
We build a transformer from the paper "Attention is all you need" from scratch for learning purposes.

In this project we implement the transformer model from the paper "Attention is all you need" that can be found at https://arxiv.org/pdf/1706.03762.pdf. This paper has been highly impactful in the field of NLP and deep learning. In it the authors proposed a novel architecture of the transformer which is entirely based on the self attention mechanism and outperformed the previous state-of-the-art models, relying on RNN architecture, on various NLP tasks. 
